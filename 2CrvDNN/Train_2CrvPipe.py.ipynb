{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "c921e1df-b8b8-4d61-af24-b83b53612de7",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "以下に、提示方針（回転拡張の無効化、正則化強化、角度損失の重み増、線形ラベルのlog1p、分割シャッフル）を反映した「そのまま実行可能な」フルコードを示します。既存のPointNet＋STN＋OneCycleLR＋EMA＋TTAを維持し、前処理（SOR・正規化・PCAアラインメント）や評価保存も含みます。\n",
    "重要\n",
    "- params_all.txt の角度が「度（deg）」なら angle_unit_labels='deg' に必ず変更してください。ラジアン前提なら 'rad' のままでOKです。\n",
    "- 角度が「グローバル座標基準」なら、回転拡張は無効化しています（augment_rotate=False）。主軸基準の角度なら canonical_align=True を試す価値があります。\n",
    "コード（Train_pipe8-2.py）\n",
    "\"\"\"\n",
    "# Train_pipe8-2.py (汎化改善版)\n",
    "# - weight_decay を 1e-3 に上げる、dropout_p を 0.5 に上げる、ft_reg_weight を 0.02 に上げる\n",
    "# - num_points を 8192 に増やす、batch_size を増やす\n",
    "import os\n",
    "import json\n",
    "import csv\n",
    "from pathlib import Path\n",
    "import random\n",
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "import matplotlib.pyplot as plt\n",
    "# ----------------\n",
    "# 乱数シード固定\n",
    "# ----------------\n",
    "def seed_everything(seed=42):\n",
    "    random.seed(seed)\n",
    "    np.random.seed(seed)\n",
    "    torch.manual_seed(seed)\n",
    "    if torch.cuda.is_available():\n",
    "        torch.cuda.manual_seed_all(seed)\n",
    "    # 完全決定論が必要なら以下も検討\n",
    "    # torch.backends.cudnn.deterministic = True\n",
    "    # torch.backends.cudnn.benchmark = False\n",
    "# ----------------\n",
    "# params_all.txt を 7次元ラベルに復元\n",
    "# ----------------\n",
    "def load_params_all_to_vec7(filename):\n",
    "    \"\"\"\n",
    "    params_all.txt（ヘッダあり）を読み込み、\n",
    "    各 case_id のラベル [L1, L2, L3, R1, R2, theta1, theta2]（7次元）を返す。\n",
    "    戻り値:\n",
    "      labels: (n_cases, 7) の numpy 配列\n",
    "      case_ids: [0..n_cases-1] のリスト\n",
    "    形式（CSVヘッダ: case_id,seg_idx,val0,val1）\n",
    "      seg_idx=0: L1\n",
    "      seg_idx=1: theta1 (val0), R1 (val1)\n",
    "      seg_idx=2: L2\n",
    "      seg_idx=3: theta2 (val0), R2 (val1)\n",
    "      seg_idx=4: L3\n",
    "    \"\"\"\n",
    "    data = np.loadtxt(filename, delimiter=\",\", skiprows=1)\n",
    "    case_ids = data[:, 0].astype(int)\n",
    "    seg_idx  = data[:, 1].astype(int)\n",
    "    val0     = data[:, 2].astype(float)\n",
    "    val1     = data[:, 3].astype(float)\n",
    "    n_cases = int(case_ids.max()) + 1\n",
    "    labels = np.zeros((n_cases, 7), dtype=np.float32)\n",
    "    for cid in range(n_cases):\n",
    "        mask = (case_ids == cid)\n",
    "        segs = seg_idx[mask]\n",
    "        v0   = val0[mask]\n",
    "        v1   = val1[mask]\n",
    "        labels[cid, 0] = v0[segs == 0][0]  # L1\n",
    "        labels[cid, 5] = v0[segs == 1][0]  # theta1（読み込み時の単位）\n",
    "        labels[cid, 3] = v1[segs == 1][0]  # R1\n",
    "        labels[cid, 1] = v0[segs == 2][0]  # L2\n",
    "        labels[cid, 6] = v0[segs == 3][0]  # theta2（読み込み時の単位）\n",
    "        labels[cid, 4] = v1[segs == 3][0]  # R2\n",
    "        labels[cid, 2] = v0[segs == 4][0]  # L3\n",
    "    return labels, list(range(n_cases))\n",
    "# ----------------\n",
    "# ラベル単位変換・整形ユーティリティ\n",
    "# ----------------\n",
    "def adjust_label_units(labels, angle_unit='rad', length_scale=1.0, wrap_angles=True):\n",
    "    \"\"\"\n",
    "    labels: (n_cases, 7) [L1,L2,L3,R1,R2,theta1,theta2]\n",
    "    angle_unit: 'rad'（そのまま） or 'deg'（度→ラジアンに変換）\n",
    "    length_scale: 長さのスケール係数（例: mm→mは 0.001）\n",
    "    wrap_angles: Trueなら角度を [-pi, pi] にラップ\n",
    "    \"\"\"\n",
    "    labels = labels.copy().astype(np.float32)\n",
    "    labels[:, :5] *= float(length_scale)\n",
    "    if angle_unit.lower() == 'deg':\n",
    "        labels[:, 5:] = labels[:, 5:] * (np.pi / 180.0)\n",
    "    if wrap_angles:\n",
    "        labels[:, 5] = (labels[:, 5] + np.pi) % (2.0 * np.pi) - np.pi\n",
    "        labels[:, 6] = (labels[:, 6] + np.pi) % (2.0 * np.pi) - np.pi\n",
    "    return labels\n",
    "# ----------------\n",
    "# Farthest Point Sampling（簡易版、numpy）\n",
    "# ----------------\n",
    "def farthest_point_sampling(pts, num_samples):\n",
    "    \"\"\"\n",
    "    pts: (N,3) numpy\n",
    "    num_samples: int\n",
    "    戻り: (num_samples,3) 選択点\n",
    "    \"\"\"\n",
    "    N = pts.shape[0]\n",
    "    if N <= num_samples:\n",
    "        reps = num_samples - N\n",
    "        extra_idx = np.random.choice(N, reps, replace=True)\n",
    "        idx = np.concatenate([np.arange(N), extra_idx], axis=0)\n",
    "        return pts[idx, :]\n",
    "    selected_idx = np.zeros((num_samples,), dtype=np.int64)\n",
    "    selected_idx[0] = np.random.randint(0, N)\n",
    "    dists = np.full((N,), np.inf, dtype=np.float32)\n",
    "    last_pt = pts[selected_idx[0]]\n",
    "    dists = np.minimum(dists, np.sum((pts - last_pt)**2, axis=1))\n",
    "    for i in range(1, num_samples):\n",
    "        next_idx = int(np.argmax(dists))\n",
    "        selected_idx[i] = next_idx\n",
    "        last_pt = pts[next_idx]\n",
    "        dists = np.minimum(dists, np.sum((pts - last_pt)**2, axis=1))\n",
    "    return pts[selected_idx, :]\n",
    "# ----------------\n",
    "# 前処理ユーティリティ（重複除去・SOR・PCAアライン・正規化）\n",
    "# ----------------\n",
    "def remove_duplicates(pts, round_decimals=6):\n",
    "    if pts.shape[0] <= 1:\n",
    "        return pts\n",
    "    rounded = np.round(pts, decimals=round_decimals)\n",
    "    _, uniq_idx = np.unique(rounded, axis=0, return_index=True)\n",
    "    return pts[np.sort(uniq_idx)]\n",
    "def statistical_outlier_removal(pts, k=16, std_ratio=2.0):\n",
    "    \"\"\"\n",
    "    統計的外れ値除去（SOR）。\n",
    "    各点のk近傍平均距離の分布を算出し、平均+std_ratio*std を超える点を外れ値とみなし除去。\n",
    "    O(N^2) 近傍距離計算（4096点程度なら前処理として許容）\n",
    "    \"\"\"\n",
    "    N = pts.shape[0]\n",
    "    if N < (k + 2):\n",
    "        return pts\n",
    "    mean_dists = np.zeros((N,), dtype=np.float32)\n",
    "    chunk = 512\n",
    "    for i in range(0, N, chunk):\n",
    "        c_end = min(i + chunk, N)\n",
    "        c_pts = pts[i:c_end]                   # (C,3)\n",
    "        d2 = np.sum((c_pts[:, None, :] - pts[None, :, :])**2, axis=2)  # (C,N)\n",
    "        k1 = k + 1\n",
    "        part = np.partition(d2, k1, axis=1)[:, :k1]  # (C, k+1)\n",
    "        part_sorted = np.sort(part, axis=1)\n",
    "        nnk = part_sorted[:, 1:]  # (C, k)\n",
    "        mean_dists[i:c_end] = np.sqrt(np.mean(nnk, axis=1)).astype(np.float32)\n",
    "    mu = float(np.mean(mean_dists))\n",
    "    sigma = float(np.std(mean_dists))\n",
    "    if sigma < 1e-9:\n",
    "        return pts\n",
    "    keep = mean_dists <= (mu + std_ratio * sigma)\n",
    "    if keep.sum() < max(32, int(0.3 * N)):\n",
    "        return pts\n",
    "    return pts[keep]\n",
    "def pca_align_to_z(pts):\n",
    "    \"\"\"\n",
    "    PCAで主軸アラインメント。最大固有値方向をZ軸に合わせ、右手系を維持。\n",
    "    返り値: (aligned_pts, R, centroid)\n",
    "    \"\"\"\n",
    "    centroid = pts.mean(axis=0, keepdims=True)\n",
    "    X = pts - centroid\n",
    "    cov = np.cov(X.T)\n",
    "    eigvals, eigvecs = np.linalg.eigh(cov)\n",
    "    order = np.argsort(eigvals)[::-1]\n",
    "    v0 = eigvecs[:, order[0]]\n",
    "    v1 = eigvecs[:, order[1]]\n",
    "    v2 = np.cross(v0, v1)\n",
    "    v1 = v1 / (np.linalg.norm(v1) + 1e-9)\n",
    "    v2 = v2 / (np.linalg.norm(v2) + 1e-9)\n",
    "    v0 = v0 / (np.linalg.norm(v0) + 1e-9)\n",
    "    R = np.stack([v1, v2, v0], axis=1).astype(np.float32)\n",
    "    aligned = X @ R\n",
    "    return aligned.astype(np.float32), R, centroid.astype(np.float32)\n",
    "def normalize_coords(pts, method='unit_sphere'):\n",
    "    method = (method or 'none').lower()\n",
    "    if method == 'unit_sphere':\n",
    "        centroid = pts.mean(axis=0, keepdims=True)\n",
    "        X = pts - centroid\n",
    "        scale = np.max(np.linalg.norm(X, axis=1))\n",
    "        scale = 1.0 if scale < 1e-9 else scale\n",
    "        return (X / scale).astype(np.float32)\n",
    "    elif method == 'zscore':\n",
    "        mu = pts.mean(axis=0, keepdims=True)\n",
    "        sigma = pts.std(axis=0, keepdims=True)\n",
    "        sigma[sigma < 1e-9] = 1.0\n",
    "        return ((pts - mu) / sigma).astype(np.float32)\n",
    "    else:\n",
    "        return pts.astype(np.float32)\n",
    "def preprocess_points_np(\n",
    "    pts,\n",
    "    coord_scale=1.0,\n",
    "    dedup_round_decimals=6,\n",
    "    use_sor=True, sor_k=16, sor_std_ratio=2.0,\n",
    "    canonical_align=False,\n",
    "    normalize_method='unit_sphere'\n",
    "):\n",
    "    \"\"\"\n",
    "    推奨の前処理パイプライン（ランダム無し）。学習・推論共通で使用。\n",
    "    1) 単位スケール（例: mm→m）\n",
    "    2) 重複点除去\n",
    "    3) 統計的外れ値除去（SOR）\n",
    "    4) PCA主軸アラインメント（任意）\n",
    "    5) 座標正規化（ユニットスフィア/Zスコア/無し）\n",
    "    \"\"\"\n",
    "    pts = pts.astype(np.float32)\n",
    "    pts = pts * float(coord_scale)\n",
    "    pts = remove_duplicates(pts, round_decimals=dedup_round_decimals)\n",
    "    if use_sor:\n",
    "        pts = statistical_outlier_removal(pts, k=int(sor_k), std_ratio=float(sor_std_ratio))\n",
    "    if canonical_align:\n",
    "        pts, _, _ = pca_align_to_z(pts)\n",
    "    pts = normalize_coords(pts, method=normalize_method)\n",
    "    return pts\n",
    "# ----------------\n",
    "# Dataset（FPS + 拡張 + 前処理キャッシュ）\n",
    "# ----------------\n",
    "class PipePointParamDataset(torch.utils.data.Dataset):\n",
    "    \"\"\"\n",
    "    - 入力: point cloud from pipe_case_{case_id:02d}.xyz\n",
    "    - ラベル: params_all.txt から復元した 7次元ベクトル\n",
    "    - augment_times: データセット長を倍増\n",
    "    - use_fps: TrueならFarthest Point Samplingでサブサンプリング\n",
    "    - 前処理はキャッシュ（SOR/PCA/正規化）\n",
    "    - 回転拡張は本コードでは無効化（角度がグローバル座標基準のため）\n",
    "    \"\"\"\n",
    "    def __init__(self, indices, labels, root_dir=\".\", num_points=4096, \n",
    "                 preprocess_cfg=None,\n",
    "                 normalize=True,\n",
    "                 augment_times=1, augment_rotate=False, rotate_axis=None,\n",
    "                 augment_scale=True, scale_low=0.98, scale_high=1.02,\n",
    "                 augment_jitter=True, jitter_std=0.002, jitter_clip=0.01,\n",
    "                 augment_dropout=True, dropout_rate=0.03,\n",
    "                 use_fps=True,\n",
    "                 pre_cache=True):\n",
    "        super().__init__()\n",
    "        self.indices = list(indices)\n",
    "        self.labels = labels\n",
    "        self.root_dir = root_dir\n",
    "        self.num_points = num_points\n",
    "        self.normalize = normalize\n",
    "        self.preprocess_cfg = preprocess_cfg or {}\n",
    "        self.coord_scale = float(self.preprocess_cfg.get(\"coord_scale\", 1.0))\n",
    "        self.dedup_round_decimals = int(self.preprocess_cfg.get(\"dedup_round_decimals\", 6))\n",
    "        self.use_sor = bool(self.preprocess_cfg.get(\"use_sor\", True))\n",
    "        self.sor_k = int(self.preprocess_cfg.get(\"sor_k\", 16))\n",
    "        self.sor_std_ratio = float(self.preprocess_cfg.get(\"sor_std_ratio\", 2.0))\n",
    "        self.canonical_align = bool(self.preprocess_cfg.get(\"canonical_align\", False))\n",
    "        self.normalize_method = str(self.preprocess_cfg.get(\"normalize_method\", \"unit_sphere\")).lower()\n",
    "        self.augment_times = int(augment_times)\n",
    "        self.augment_rotate = bool(augment_rotate)\n",
    "        self.rotate_axis = rotate_axis\n",
    "        self.augment_scale = bool(augment_scale)\n",
    "        self.scale_low = float(scale_low)\n",
    "        self.scale_high = float(scale_high)\n",
    "        self.augment_jitter = bool(augment_jitter)\n",
    "        self.jitter_std = float(jitter_std)\n",
    "        self.jitter_clip = float(jitter_clip)\n",
    "        self.augment_dropout = bool(augment_dropout)\n",
    "        self.dropout_rate = float(dropout_rate)\n",
    "        self.use_fps = bool(use_fps)\n",
    "        self.base_len = len(self.indices)\n",
    "        self.pre_cache = bool(pre_cache)\n",
    "        self._cache = {}\n",
    "        if self.pre_cache:\n",
    "            self._build_cache()\n",
    "    def __len__(self):\n",
    "        return self.base_len * self.augment_times\n",
    "    def _load_points(self, cid):\n",
    "        path = os.path.join(self.root_dir, f\"pipe_case_{cid:02d}.xyz\")\n",
    "        try:\n",
    "            pts = np.loadtxt(path, skiprows=1)\n",
    "        except Exception:\n",
    "            pts = np.loadtxt(path)\n",
    "        if pts.ndim != 2 or pts.shape[1] != 3:\n",
    "            raise ValueError(f\"Invalid point file shape: {path}, got {pts.shape}\")\n",
    "        return pts.astype(np.float32)\n",
    "    def _build_cache(self):\n",
    "        print(\"[Info] Building preprocessed cache...\")\n",
    "        for cid in self.indices:\n",
    "            pts_raw = self._load_points(cid)\n",
    "            pts_pp = preprocess_points_np(\n",
    "                pts_raw,\n",
    "                coord_scale=self.coord_scale,\n",
    "                dedup_round_decimals=self.dedup_round_decimals,\n",
    "                use_sor=self.use_sor, sor_k=self.sor_k, sor_std_ratio=self.sor_std_ratio,\n",
    "                canonical_align=self.canonical_align,\n",
    "                normalize_method=self.normalize_method\n",
    "            )\n",
    "            self._cache[cid] = pts_pp\n",
    "        print(f\"[Info] Preprocessed cache ready for {len(self._cache)} cases.\")\n",
    "    def _subsample_or_tile(self, pts):\n",
    "        N = pts.shape[0]\n",
    "        if N >= self.num_points:\n",
    "            if self.use_fps:\n",
    "                return farthest_point_sampling(pts, self.num_points)\n",
    "            else:\n",
    "                idx = np.random.choice(N, self.num_points, replace=False)\n",
    "                return pts[idx, :]\n",
    "        else:\n",
    "            reps = self.num_points - N\n",
    "            extra_idx = np.random.choice(N, reps, replace=True)\n",
    "            idx = np.concatenate([np.arange(N), extra_idx], axis=0)\n",
    "            return pts[idx, :]\n",
    "    def _rotation_matrix_z(self, theta):\n",
    "        c, s = np.cos(theta), np.sin(theta)\n",
    "        R = np.array([[c, -s, 0.0],\n",
    "                      [s,  c, 0.0],\n",
    "                      [0.0, 0.0, 1.0]], dtype=np.float32)\n",
    "        return R\n",
    "    def _random_rotation_matrix_any(self):\n",
    "        axis = np.random.randn(3).astype(np.float32)\n",
    "        norm = np.linalg.norm(axis)\n",
    "        axis = axis / (norm + 1e-9)\n",
    "        theta = np.random.uniform(0.0, 2.0 * np.pi)\n",
    "        K = np.array([[0, -axis[2], axis[1]],\n",
    "                      [axis[2], 0, -axis[0]],\n",
    "                      [-axis[1], axis[0], 0]], dtype=np.float32)\n",
    "        I = np.eye(3, dtype=np.float32)\n",
    "        R = I + np.sin(theta) * K + (1.0 - np.cos(theta)) * (K @ K)\n",
    "        return R\n",
    "    def _apply_random_rotation(self, pts):\n",
    "        # 本コードでは augment_rotate=False を推奨（角度がグローバル基準のため）\n",
    "        if not self.augment_rotate:\n",
    "            return pts\n",
    "        if self.rotate_axis == 'z':\n",
    "            theta = np.random.uniform(0.0, 2.0*np.pi)\n",
    "            R = self._rotation_matrix_z(theta)\n",
    "        elif self.rotate_axis == 'random':\n",
    "            R = self._random_rotation_matrix_any()\n",
    "        else:\n",
    "            return pts\n",
    "        return pts @ R.T\n",
    "    def _apply_random_scaling(self, pts):\n",
    "        s = np.random.uniform(self.scale_low, self.scale_high)\n",
    "        return pts * s\n",
    "    def _apply_jitter(self, pts):\n",
    "        noise = np.clip(self.jitter_std * np.random.randn(*pts.shape), -self.jitter_clip, self.jitter_clip)\n",
    "        return (pts + noise).astype(np.float32)\n",
    "    def _apply_point_dropout(self, pts):\n",
    "        N = pts.shape[0]\n",
    "        drop_idx = np.random.rand(N) < self.dropout_rate\n",
    "        if np.any(drop_idx):\n",
    "            pts[drop_idx] = pts[0]\n",
    "        return pts\n",
    "    def __getitem__(self, i):\n",
    "        base_i = i % self.base_len\n",
    "        cid = self.indices[base_i]\n",
    "        pts = self._cache[cid] if self.pre_cache else self._load_points(cid)\n",
    "        if not self.pre_cache:\n",
    "            pts = preprocess_points_np(\n",
    "                pts,\n",
    "                coord_scale=self.coord_scale,\n",
    "                dedup_round_decimals=self.dedup_round_decimals,\n",
    "                use_sor=self.use_sor, sor_k=self.sor_k, sor_std_ratio=self.sor_std_ratio,\n",
    "                canonical_align=self.canonical_align,\n",
    "                normalize_method=self.normalize_method\n",
    "            )\n",
    "        pts = self._subsample_or_tile(pts)\n",
    "        # ランダム拡張（回転は無効化）\n",
    "        pts = self._apply_random_rotation(pts)\n",
    "        if self.augment_scale:\n",
    "            pts = self._apply_random_scaling(pts)\n",
    "        if self.augment_jitter:\n",
    "            pts = self._apply_jitter(pts)\n",
    "        if self.augment_dropout:\n",
    "            pts = self._apply_point_dropout(pts)\n",
    "        pts = torch.from_numpy(pts.astype(np.float32)).transpose(0,1)  # (3, num_points)\n",
    "        label = torch.from_numpy(self.labels[cid])                      # (7,)\n",
    "        return pts, label\n",
    "# ----------------\n",
    "# STN（PointNet）\n",
    "# ----------------\n",
    "class STN3d(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(STN3d, self).__init__()\n",
    "        self.conv1 = nn.Conv1d(3, 64, 1)\n",
    "        self.conv2 = nn.Conv1d(64, 128, 1)\n",
    "        self.conv3 = nn.Conv1d(128, 1024, 1)\n",
    "        self.bn1 = nn.BatchNorm1d(64)\n",
    "        self.bn2 = nn.BatchNorm1d(128)\n",
    "        self.bn3 = nn.BatchNorm1d(1024)\n",
    "        self.fc1 = nn.Linear(1024, 512)\n",
    "        self.fc2 = nn.Linear(512, 256)\n",
    "        self.fc3 = nn.Linear(256, 9)\n",
    "        self.bn4 = nn.BatchNorm1d(512)\n",
    "        self.bn5 = nn.BatchNorm1d(256)\n",
    "    def forward(self, x):  # x: (B,3,N)\n",
    "        b = x.size(0)\n",
    "        x = F.relu(self.bn1(self.conv1(x)))\n",
    "        x = F.relu(self.bn2(self.conv2(x)))\n",
    "        x = F.relu(self.bn3(self.conv3(x)))  # (B,1024,N)\n",
    "        x = torch.max(x, 2)[0]               # (B,1024)\n",
    "        x = F.relu(self.bn4(self.fc1(x)))\n",
    "        x = F.relu(self.bn5(self.fc2(x)))\n",
    "        x = self.fc3(x)\n",
    "        iden = torch.eye(3, dtype=torch.float32, device=x.device).view(1, 9).repeat(b, 1)\n",
    "        x = x + iden\n",
    "        x = x.view(-1, 3, 3)\n",
    "        return x\n",
    "class STNkd(nn.Module):\n",
    "    def __init__(self, k=64):\n",
    "        super(STNkd, self).__init__()\n",
    "        self.k = k\n",
    "        self.conv1 = nn.Conv1d(k, 64, 1)\n",
    "        self.conv2 = nn.Conv1d(64, 128, 1)\n",
    "        self.conv3 = nn.Conv1d(128, 1024, 1)\n",
    "        self.bn1 = nn.BatchNorm1d(64)\n",
    "        self.bn2 = nn.BatchNorm1d(128)\n",
    "        self.bn3 = nn.BatchNorm1d(1024)\n",
    "        self.fc1 = nn.Linear(1024, 512)\n",
    "        self.fc2 = nn.Linear(512, 256)\n",
    "        self.fc3 = nn.Linear(256, k * k)\n",
    "        self.bn4 = nn.BatchNorm1d(512)\n",
    "        self.bn5 = nn.BatchNorm1d(256)\n",
    "    def forward(self, x):  # x: (B,k,N)\n",
    "        b = x.size(0)\n",
    "        x = F.relu(self.bn1(self.conv1(x)))\n",
    "        x = F.relu(self.bn2(self.conv2(x)))\n",
    "        x = F.relu(self.bn3(self.conv3(x)))\n",
    "        x = torch.max(x, 2)[0]\n",
    "        x = F.relu(self.bn4(self.fc1(x)))\n",
    "        x = F.relu(self.bn5(self.fc2(x)))\n",
    "        x = self.fc3(x)\n",
    "        iden = torch.eye(self.k, dtype=torch.float32, device=x.device).view(1, self.k * self.k).repeat(b, 1)\n",
    "        x = x + iden\n",
    "        x = x.view(-1, self.k, self.k)\n",
    "        return x\n",
    "def feature_transform_regularizer(trans):\n",
    "    k = trans.size(1)\n",
    "    I = torch.eye(k, device=trans.device).unsqueeze(0).expand(trans.size(0), -1, -1)\n",
    "    loss = torch.mean(torch.norm(torch.bmm(trans, trans.transpose(2, 1)) - I, dim=(1, 2)))\n",
    "    return loss\n",
    "# ----------------\n",
    "# Model (PointNet + STN + BN + Dropout)\n",
    "# ----------------\n",
    "class PointNetBackbone(nn.Module):\n",
    "    def __init__(self, use_feature_stn=True, dropout_p=0.5):\n",
    "        super(PointNetBackbone, self).__init__()\n",
    "        self.stn = STN3d()\n",
    "        self.use_feature_stn = use_feature_stn\n",
    "        if use_feature_stn:\n",
    "            self.fstn = STNkd(k=64)\n",
    "        self.conv1 = nn.Conv1d(3, 64, 1)\n",
    "        self.conv2 = nn.Conv1d(64, 128, 1)\n",
    "        self.conv3 = nn.Conv1d(128, 1024, 1)\n",
    "        self.bn1 = nn.BatchNorm1d(64)\n",
    "        self.bn2 = nn.BatchNorm1d(128)\n",
    "        self.bn3 = nn.BatchNorm1d(1024)\n",
    "        self.dropout_p = dropout_p\n",
    "    def forward(self, x):  # x: (B,3,N)\n",
    "        B, _, N = x.size()\n",
    "        trans = self.stn(x)  # (B,3,3)\n",
    "        x_t = x.transpose(2, 1)  # (B,N,3)\n",
    "        x_t = torch.bmm(x_t, trans)\n",
    "        x = x_t.transpose(2, 1)  # (B,3,N)\n",
    "        x = F.relu(self.bn1(self.conv1(x)))  # (B,64,N)\n",
    "        trans_feat = None\n",
    "        if self.use_feature_stn:\n",
    "            trans_feat = self.fstn(x)        # (B,64,64)\n",
    "            x_t = x.transpose(2, 1)          # (B,N,64)\n",
    "            x_t = torch.bmm(x_t, trans_feat) # (B,N,64)\n",
    "            x = x_t.transpose(2, 1)          # (B,64,N)\n",
    "        x = F.relu(self.bn2(self.conv2(x)))  # (B,128,N)\n",
    "        x = F.relu(self.bn3(self.conv3(x)))  # (B,1024,N)\n",
    "        x = torch.max(x, 2)[0]               # (B,1024)\n",
    "        return x, trans, trans_feat\n",
    "class PipeDimensionRegressor(nn.Module):\n",
    "    def __init__(self, out_dim=9, dropout_p=0.5, use_feature_stn=True):\n",
    "        super(PipeDimensionRegressor, self).__init__()\n",
    "        self.backbone = PointNetBackbone(use_feature_stn=use_feature_stn, dropout_p=dropout_p)\n",
    "        self.fc1 = nn.Linear(1024, 512)\n",
    "        self.bn1 = nn.BatchNorm1d(512)\n",
    "        self.fc2 = nn.Linear(512, 256)\n",
    "        self.bn2 = nn.BatchNorm1d(256)\n",
    "        self.dropout = nn.Dropout(dropout_p)\n",
    "        self.fc_out = nn.Linear(256, out_dim)  # 9-dim: [L1,L2,L3,R1,R2,s1,c1,s2,c2]\n",
    "        self._init_weights()\n",
    "    def _init_weights(self):\n",
    "        for m in [self.fc1, self.fc2, self.fc_out]:\n",
    "            nn.init.kaiming_normal_(m.weight, nonlinearity='relu')\n",
    "            nn.init.zeros_(m.bias)\n",
    "    def forward(self, x):  # x: (B,3,N)\n",
    "        feat, trans, trans_feat = self.backbone(x)\n",
    "        x = F.relu(self.bn1(self.fc1(feat)))\n",
    "        x = F.relu(self.bn2(self.fc2(x)))\n",
    "        x = self.dropout(x)\n",
    "        out = self.fc_out(x)\n",
    "        return out, trans, trans_feat\n",
    "# ----------------\n",
    "# ラベル標準化（線形5次元のみ + 任意でlog1p）\n",
    "# ----------------\n",
    "class LabelScaler:\n",
    "    \"\"\"\n",
    "    先頭5次元（[L1,L2,L3,R1,R2]）のみ標準化パラメータを保持\n",
    "    use_log_linear: Trueなら loss計算時に log1p を適用（動的レンジの圧縮）\n",
    "    \"\"\"\n",
    "    def __init__(self, use_log_linear=True):\n",
    "        self.mean5 = None\n",
    "        self.std5 = None\n",
    "        self.use_log_linear = bool(use_log_linear)\n",
    "    def _prep(self, arr5):\n",
    "        if self.use_log_linear:\n",
    "            return np.log1p(np.maximum(arr5, 0.0))\n",
    "        return arr5\n",
    "    def fit(self, y_train: np.ndarray):\n",
    "        arr5 = self._prep(y_train[:, :5])\n",
    "        mean = arr5.mean(axis=0)\n",
    "        std = arr5.std(axis=0)\n",
    "        std[std < 1e-8] = 1.0\n",
    "        self.mean5 = mean.astype(np.float32)\n",
    "        self.std5 = std.astype(np.float32)\n",
    "        return self\n",
    "    def to_torch(self, device):\n",
    "        mean_t = torch.from_numpy(self.mean5).to(device)  # (5,)\n",
    "        std_t = torch.from_numpy(self.std5).to(device)    # (5,)\n",
    "        return mean_t, std_t\n",
    "# ----------------\n",
    "# 角度ユーティリティ\n",
    "# ----------------\n",
    "def angles_to_sincos(thetas: torch.Tensor):\n",
    "    s1 = torch.sin(thetas[:, 0:1])\n",
    "    c1 = torch.cos(thetas[:, 0:1])\n",
    "    s2 = torch.sin(thetas[:, 1:2])\n",
    "    c2 = torch.cos(thetas[:, 1:2])\n",
    "    return torch.cat([s1, c1, s2, c2], dim=1)\n",
    "def normalize_pairwise_sincos(sincos: torch.Tensor):\n",
    "    v1 = sincos[:, 0:2]\n",
    "    v2 = sincos[:, 2:4]\n",
    "    v1 = v1 / torch.clamp(v1.norm(dim=1, keepdim=True), min=1e-6)\n",
    "    v2 = v2 / torch.clamp(v2.norm(dim=1, keepdim=True), min=1e-6)\n",
    "    return torch.cat([v1, v2], dim=1)\n",
    "def wrap_angle_diff(a, b):\n",
    "    d = a - b\n",
    "    return (d + np.pi) % (2.0 * np.pi) - np.pi\n",
    "# ----------------\n",
    "# EarlyStopping\n",
    "# ----------------\n",
    "class EarlyStopping:\n",
    "    def __init__(self, patience=300, min_delta=0.0):\n",
    "        self.patience = patience\n",
    "        self.min_delta = min_delta\n",
    "        self.best = float(\"inf\")\n",
    "        self.wait = 0\n",
    "        self.best_state = None\n",
    "        self.best_epoch = 0\n",
    "    def step(self, metric, model, epoch):\n",
    "        improved = (self.best - metric) > self.min_delta\n",
    "        if improved:\n",
    "            self.best = metric\n",
    "            self.wait = 0\n",
    "            self.best_state = {k: v.detach().cpu().clone() for k, v in model.state_dict().items()}\n",
    "            self.best_epoch = epoch\n",
    "        else:\n",
    "            self.wait += 1\n",
    "        return self.wait > self.patience\n",
    "# ----------------\n",
    "# EMA（指数移動平均）\n",
    "# ----------------\n",
    "class ModelEMA:\n",
    "    def __init__(self, model, decay=0.999):\n",
    "        self.decay = decay\n",
    "        self.device = next(model.parameters()).device\n",
    "        self.ema = self._clone_model(model).to(self.device)\n",
    "        self._hard_sync(model)\n",
    "    @torch.no_grad()\n",
    "    def _clone_model(self, model):\n",
    "        ema = type(model)(out_dim=model.fc_out.out_features, dropout_p=model.dropout.p, use_feature_stn=True)\n",
    "        ema.load_state_dict(model.state_dict(), strict=True)\n",
    "        for p in ema.parameters():\n",
    "            p.requires_grad_(False)\n",
    "        return ema\n",
    "    @torch.no_grad()\n",
    "    def _hard_sync(self, model):\n",
    "        for ema_p, src_p in zip(self.ema.parameters(), model.parameters()):\n",
    "            ema_p.data.copy_(src_p.data)\n",
    "        for ema_b, src_b in zip(self.ema.buffers(), model.buffers()):\n",
    "            ema_b.data.copy_(src_b.data)\n",
    "    @torch.no_grad()\n",
    "    def update(self, model):\n",
    "        for ema_p, src_p in zip(self.ema.parameters(), model.parameters()):\n",
    "            ema_p.data.mul_(self.decay).add_(src_p.data, alpha=(1.0 - self.decay))\n",
    "        for ema_b, src_b in zip(self.ema.buffers(), model.buffers()):\n",
    "            ema_b.data.copy_(src_b.data)\n",
    "    def state_dict(self):\n",
    "        return self.ema.state_dict()\n",
    "# ----------------\n",
    "# Train / Eval（カスタム損失: 線形5次元 + 角度sincos + STN正則化）\n",
    "# ----------------\n",
    "def compute_losses(outputs, targets, label_mu5, label_sigma5, criterion_linear, criterion_angle,\n",
    "                   trans_feat=None, ft_reg_weight=0.01, angle_weight=2.5, use_log_linear=True):\n",
    "    \"\"\"\n",
    "    outputs: (B,9) -> [L1..R2,s1,c1,s2,c2]\n",
    "    targets: (B,7) -> [L1..R2,theta1,theta2]\n",
    "    \"\"\"\n",
    "    # 線形5次元\n",
    "    out_lin = outputs[:, :5]\n",
    "    tgt_lin = targets[:, :5]\n",
    "    if use_log_linear:\n",
    "        out_lin_t = torch.log1p(torch.clamp(out_lin, min=0.0))\n",
    "        tgt_lin_t = torch.log1p(torch.clamp(tgt_lin, min=0.0))\n",
    "    else:\n",
    "        out_lin_t = out_lin\n",
    "        tgt_lin_t = tgt_lin\n",
    "    out_lin_norm = (out_lin_t - label_mu5) / label_sigma5\n",
    "    tgt_lin_norm = (tgt_lin_t - label_mu5) / label_sigma5\n",
    "    loss_lin = criterion_linear(out_lin_norm, tgt_lin_norm)\n",
    "    # 角度（sincos）\n",
    "    out_sc = outputs[:, 5:9]\n",
    "    out_sc = normalize_pairwise_sincos(out_sc)\n",
    "    tgt_sc = angles_to_sincos(targets[:, 5:7])\n",
    "    loss_ang = criterion_angle(out_sc, tgt_sc)\n",
    "    loss = loss_lin + angle_weight * loss_ang\n",
    "    # STN feature transform 正則化\n",
    "    reg = 0.0\n",
    "    if trans_feat is not None:\n",
    "        reg = feature_transform_regularizer(trans_feat) * ft_reg_weight\n",
    "        loss = loss + reg\n",
    "    return loss, loss_lin.detach(), loss_ang.detach(), (reg if isinstance(reg, torch.Tensor) else torch.tensor(reg))\n",
    "def train_one_epoch(model, optimizer, dataloader, device,\n",
    "                    label_mu5, label_sigma5,\n",
    "                    criterion_linear, criterion_angle,\n",
    "                    scaler=None,\n",
    "                    max_grad_norm=1.0, ft_reg_weight=0.01, angle_weight=2.5,\n",
    "                    ema_updater=None,\n",
    "                    use_log_linear=True):\n",
    "    model.train()\n",
    "    total_loss = 0.0\n",
    "    for inputs, targets in dataloader:\n",
    "        inputs = inputs.to(device)    # (B,3,N)\n",
    "        targets = targets.to(device)  # (B,7)\n",
    "        optimizer.zero_grad(set_to_none=True)\n",
    "        with torch.cuda.amp.autocast(enabled=(scaler is not None)):\n",
    "            outputs, _, trans_feat = model(inputs)  # outputs: (B,9)\n",
    "            loss, _, _, _ = compute_losses(outputs, targets, label_mu5, label_sigma5,\n",
    "                                           criterion_linear, criterion_angle,\n",
    "                                           trans_feat=trans_feat, ft_reg_weight=ft_reg_weight, angle_weight=angle_weight,\n",
    "                                           use_log_linear=use_log_linear)\n",
    "        if scaler is not None:\n",
    "            scaler.scale(loss).backward()\n",
    "            if max_grad_norm is not None:\n",
    "                scaler.unscale_(optimizer)\n",
    "                torch.nn.utils.clip_grad_norm_(model.parameters(), max_norm=max_grad_norm)\n",
    "            scaler.step(optimizer)\n",
    "            scaler.update()\n",
    "        else:\n",
    "            loss.backward()\n",
    "            if max_grad_norm is not None:\n",
    "                torch.nn.utils.clip_grad_norm_(model.parameters(), max_norm=max_grad_norm)\n",
    "            optimizer.step()\n",
    "        if ema_updater is not None:\n",
    "            ema_updater.update(model)\n",
    "        total_loss += loss.item() * inputs.size(0)\n",
    "    return total_loss / len(dataloader.dataset)\n",
    "@torch.no_grad()\n",
    "def evaluate(eval_model, dataloader, device,\n",
    "             label_mu5, label_sigma5,\n",
    "             criterion_linear, criterion_angle,\n",
    "             ft_reg_weight=0.01, angle_weight=2.5,\n",
    "             use_log_linear=True):\n",
    "    eval_model.eval()\n",
    "    total_loss = 0.0\n",
    "    for inputs, targets in dataloader:\n",
    "        inputs = inputs.to(device)\n",
    "        targets = targets.to(device)\n",
    "        outputs, _, trans_feat = eval_model(inputs)\n",
    "        loss, _, _, _ = compute_losses(outputs, targets, label_mu5, label_sigma5,\n",
    "                                       criterion_linear, criterion_angle,\n",
    "                                       trans_feat=trans_feat, ft_reg_weight=ft_reg_weight, angle_weight=angle_weight,\n",
    "                                       use_log_linear=use_log_linear)\n",
    "        total_loss += loss.item() * inputs.size(0)\n",
    "    return total_loss / len(dataloader.dataset)\n",
    "# ----------------\n",
    "# 評価保存ユーティリティ（角度を元に戻して評価、TTA＋前処理対応）\n",
    "# ----------------\n",
    "def compute_metrics(preds_np, trues_np, angle_indices=(5,6), deg=False):\n",
    "    err = preds_np - trues_np\n",
    "    if angle_indices is not None:\n",
    "        for ai in angle_indices:\n",
    "            err[:, ai] = wrap_angle_diff(preds_np[:, ai], trues_np[:, ai])\n",
    "    if deg:\n",
    "        scale = np.ones(err.shape[1], dtype=np.float32)\n",
    "        for ai in angle_indices:\n",
    "            scale[ai] = 180.0 / np.pi\n",
    "        err = err * scale\n",
    "    mae = np.mean(np.abs(err), axis=0)\n",
    "    mse = np.mean(err**2, axis=0)\n",
    "    rmse = np.sqrt(mse)\n",
    "    overall = {\n",
    "        \"MAE_mean\": float(np.mean(np.abs(err))),\n",
    "        \"MSE_mean\": float(np.mean(err**2)),\n",
    "        \"RMSE_mean\": float(np.sqrt(np.mean(err**2)))\n",
    "    }\n",
    "    keys = [\"L1\",\"L2\",\"L3\",\"R1\",\"R2\",\"theta1\",\"theta2\"]\n",
    "    by_dim = {k: {\"MAE\": float(mae[i]), \"MSE\": float(mse[i]), \"RMSE\": float(rmse[i])} for i, k in enumerate(keys)}\n",
    "    return overall, by_dim\n",
    "@torch.no_grad()\n",
    "def _predict7_from_points(model, pts_np, device, num_points=4096,\n",
    "                          preprocess_cfg=None, tta_times=8, rotate_axis=None):\n",
    "    \"\"\"\n",
    "    1つの点群に対して、TTA（複数サブサンプリング）で予測平均を返す。\n",
    "    model: PipeDimensionRegressor（out_dim=9）互換\n",
    "    戻り: (7,) numpy [L1,L2,L3,R1,R2,theta1,theta2]\n",
    "    \"\"\"\n",
    "    preprocess_cfg = preprocess_cfg or {}\n",
    "    preds = []\n",
    "    for _ in range(tta_times):\n",
    "        pts = pts_np.copy().astype(np.float32)\n",
    "        # 前処理\n",
    "        pts = preprocess_points_np(\n",
    "            pts,\n",
    "            coord_scale=float(preprocess_cfg.get(\"coord_scale\", 1.0)),\n",
    "            dedup_round_decimals=int(preprocess_cfg.get(\"dedup_round_decimals\", 6)),\n",
    "            use_sor=bool(preprocess_cfg.get(\"use_sor\", True)),\n",
    "            sor_k=int(preprocess_cfg.get(\"sor_k\", 16)),\n",
    "            sor_std_ratio=float(preprocess_cfg.get(\"sor_std_ratio\", 2.0)),\n",
    "            canonical_align=bool(preprocess_cfg.get(\"canonical_align\", False)),\n",
    "            normalize_method=str(preprocess_cfg.get(\"normalize_method\", \"unit_sphere\")).lower()\n",
    "        )\n",
    "        # サブサンプル\n",
    "        pts = farthest_point_sampling(pts, num_points)\n",
    "        # 任意の回転TTA（Valでは通常None）\n",
    "        if rotate_axis == 'z':\n",
    "            theta = np.random.uniform(0.0, 2.0*np.pi)\n",
    "            c, s = np.cos(theta), np.sin(theta)\n",
    "            R = np.array([[c, -s, 0.0],[s, c, 0.0],[0.0,0.0,1.0]], dtype=np.float32)\n",
    "            pts = pts @ R.T\n",
    "        x = torch.from_numpy(pts.astype(np.float32)).transpose(0,1).unsqueeze(0).to(device)  # (1,3,N)\n",
    "        out9, _, _ = model(x)\n",
    "        out9 = out9.cpu().numpy().reshape(-1)\n",
    "        pred_lin = out9[:5]\n",
    "        s1, c1, s2, c2 = out9[5], out9[6], out9[7], out9[8]\n",
    "        n1 = np.linalg.norm([s1, c1]); n1 = 1.0 if n1 < 1e-6 else n1\n",
    "        n2 = np.linalg.norm([s2, c2]); n2 = 1.0 if n2 < 1e-6 else n2\n",
    "        s1, c1 = s1 / n1, c1 / n1\n",
    "        s2, c2 = s2 / n2, c2 / n2\n",
    "        theta1 = np.arctan2(s1, c1)\n",
    "        theta2 = np.arctan2(s2, c2)\n",
    "        preds.append(np.array([pred_lin[0], pred_lin[1], pred_lin[2], pred_lin[3], pred_lin[4], theta1, theta2], dtype=np.float32))\n",
    "    return np.mean(np.stack(preds, axis=0), axis=0)\n",
    "def evaluate_and_save_casewise(model, val_case_ids, labels, device,\n",
    "                               root_dir=\".\", num_points=4096,\n",
    "                               out_csv=\"results8-2/val_predictions.csv\",\n",
    "                               out_json=\"results8-2/val_metrics.json\",\n",
    "                               tta_times=8, rotate_axis=None,\n",
    "                               preprocess_cfg=None, save_degree_metrics=True):\n",
    "    Path(out_csv).parent.mkdir(parents=True, exist_ok=True)\n",
    "    preds_all, trues_all = [], []\n",
    "    header = [\"case_id\",\n",
    "              \"pred_L1\",\"pred_L2\",\"pred_L3\",\"pred_R1\",\"pred_R2\",\"pred_theta1\",\"pred_theta2\",\n",
    "              \"true_L1\",\"true_L2\",\"true_L3\",\"true_R1\",\"true_R2\",\"true_theta1\",\"true_theta2\",\n",
    "              \"abserr_L1\",\"abserr_L2\",\"abserr_L3\",\"abserr_R1\",\"abserr_R2\",\"abserr_theta1\",\"abserr_theta2\"]\n",
    "    with open(out_csv, \"w\", newline=\"\") as f:\n",
    "        writer = csv.writer(f)\n",
    "        writer.writerow(header)\n",
    "        model.eval()\n",
    "        with torch.no_grad():\n",
    "            for cid in val_case_ids:\n",
    "                path = os.path.join(root_dir, f\"pipe_case_{cid:02d}.xyz\")\n",
    "                try:\n",
    "                    pts = np.loadtxt(path, skiprows=1)\n",
    "                except Exception:\n",
    "                    pts = np.loadtxt(path)\n",
    "                if pts.ndim != 2 or pts.shape[1] != 3:\n",
    "                    raise ValueError(f\"Invalid point file shape: {path}, got {pts.shape}\")\n",
    "                pred7 = _predict7_from_points(model, pts.astype(np.float32),\n",
    "                                              device=device, num_points=num_points,\n",
    "                                              preprocess_cfg=preprocess_cfg, tta_times=tta_times, rotate_axis=rotate_axis)\n",
    "                true7 = labels[cid].astype(np.float32)\n",
    "                abs_err = np.abs(pred7 - true7)\n",
    "                abs_err[5] = np.abs(wrap_angle_diff(pred7[5], true7[5]))\n",
    "                abs_err[6] = np.abs(wrap_angle_diff(pred7[6], true7[6]))\n",
    "                preds_all.append(pred7)\n",
    "                trues_all.append(true7)\n",
    "                row = [cid] + pred7.tolist() + true7.tolist() + abs_err.tolist()\n",
    "                writer.writerow(row)\n",
    "    preds_all = np.vstack(preds_all)\n",
    "    trues_all = np.vstack(trues_all)\n",
    "    overall_rad, by_dim_rad = compute_metrics(preds_all, trues_all, angle_indices=(5,6), deg=False)\n",
    "    metrics = {\"overall_rad\": overall_rad, \"by_dim_rad\": by_dim_rad, \"num_cases\": int(len(val_case_ids))}\n",
    "    if save_degree_metrics:\n",
    "        overall_deg, by_dim_deg = compute_metrics(preds_all, trues_all, angle_indices=(5,6), deg=True)\n",
    "        metrics[\"overall_deg\"] = overall_deg\n",
    "        metrics[\"by_dim_deg\"] = by_dim_deg\n",
    "    with open(out_json, \"w\") as jf:\n",
    "        json.dump(metrics, jf, indent=2)\n",
    "    print(f\"Saved case-wise predictions to {out_csv}\")\n",
    "    print(f\"Saved metrics summary to {out_json}\")\n",
    "# ----------------\n",
    "# Main（汎化改善セット）\n",
    "# ----------------\n",
    "def main():\n",
    "    seed_everything(42)\n",
    "    device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "    results_dir = \"results8-2\"\n",
    "    Path(results_dir).mkdir(parents=True, exist_ok=True)\n",
    "    # ====== 単位・前処理の設定 ======\n",
    "    # 角度ラベル単位: 'rad' or 'deg'（params_all.txt の角度に合わせる）\n",
    "    angle_unit_labels = 'rad'  # params_all.txt が度なら 'deg' に必ず変更\n",
    "    # 長さラベルスケール（例: mm→m は 0.001）\n",
    "    label_length_scale = 1.0\n",
    "    # 座標スケール（点群座標の単位変換、例: mm→m は 0.001）\n",
    "    coord_scale = 1.0\n",
    "    # PCA主軸アラインメント（角度が主軸基準で定義されているときのみ True 推奨）\n",
    "    canonical_align = False\n",
    "    # 座標正規化メソッド: 'unit_sphere' / 'zscore' / 'none'\n",
    "    normalize_method = 'unit_sphere'\n",
    "    # SOR設定（外れ値除去）\n",
    "    use_sor = True\n",
    "    sor_k = 16\n",
    "    sor_std_ratio = 2.0\n",
    "    preprocess_cfg = {\n",
    "        \"coord_scale\": coord_scale,\n",
    "        \"dedup_round_decimals\": 6,\n",
    "        \"use_sor\": use_sor,\n",
    "        \"sor_k\": sor_k,\n",
    "        \"sor_std_ratio\": sor_std_ratio,\n",
    "        \"canonical_align\": canonical_align,\n",
    "        \"normalize_method\": normalize_method,\n",
    "    }\n",
    "    # ====== ラベルの読み込み＋単位変換 ======\n",
    "    labels_raw, case_ids = load_params_all_to_vec7(\"params_all.txt\")\n",
    "    labels = adjust_label_units(labels_raw, angle_unit=angle_unit_labels,\n",
    "                                length_scale=label_length_scale, wrap_angles=True)\n",
    "    # ====== 分割のシャッフル（固定シードで再現性を確保） ======\n",
    "    rng = np.random.default_rng(42)\n",
    "    case_ids_shuffled = np.array(case_ids)\n",
    "    rng.shuffle(case_ids_shuffled)\n",
    "    # データが少ないため Val は2件程度に抑え、Trainはなるべく多く\n",
    "    train_indices = case_ids_shuffled[:8]   # 8件学習（可能ならさらに増やす）\n",
    "    val_indices   = case_ids_shuffled[8:10] # 2件検証\n",
    "    # ====== ハイパーパラメータ（過学習抑制・角度重視） ======\n",
    "    # - weight_decay を 1e-3 に上げる、dropout_p を 0.5 に上げる、ft_reg_weight を 0.02 に上げる\n",
    "    # - num_points を 8192 に増やす、batch_size を増やす\n",
    "    batch_size    = 32\n",
    "    num_points    = 8192\n",
    "    num_epochs    = 2000            # 10000は長すぎ。OneCycleLRなら2000程度で十分\n",
    "    max_lr        = 1.0e-3\n",
    "    weight_decay  = 1.0e-3          # 正則化強化\n",
    "    max_grad_norm = 1.0\n",
    "    patience_es   = 300             # 早めに止める\n",
    "    ft_reg_weight = 0.02            # STN正則化強化\n",
    "    angle_weight  = 2.5             # 角度の比重アップ\n",
    "    use_log_linear = True           # 線形ラベルの log1p を有効化\n",
    "    # ====== データセット（回転拡張を無効化、他は控えめ） ======\n",
    "    train_dataset = PipePointParamDataset(\n",
    "        train_indices, labels, root_dir=\".\", num_points=num_points,\n",
    "        preprocess_cfg=preprocess_cfg,\n",
    "        augment_times=30,                     # データ少ないので拡張回数を増やす\n",
    "        augment_rotate=False, rotate_axis=None,  # ★ 角度ラベルがグローバル依存なら回転拡張は禁止\n",
    "        augment_scale=True, scale_low=0.98, scale_high=1.02,  # スケール控えめ\n",
    "        augment_jitter=True, jitter_std=0.002, jitter_clip=0.01,\n",
    "        augment_dropout=True, dropout_rate=0.03,\n",
    "        use_fps=True,\n",
    "        pre_cache=True\n",
    "    )\n",
    "    val_dataset   = PipePointParamDataset(\n",
    "        val_indices, labels, root_dir=\".\", num_points=num_points,\n",
    "        preprocess_cfg=preprocess_cfg,\n",
    "        augment_times=1, augment_rotate=False, rotate_axis=None,\n",
    "        augment_scale=False, augment_jitter=False, augment_dropout=False,\n",
    "        use_fps=True,\n",
    "        pre_cache=True\n",
    "    )\n",
    "    train_loader = torch.utils.data.DataLoader(train_dataset, batch_size=batch_size, shuffle=True,  drop_last=False, num_workers=0)\n",
    "    val_loader   = torch.utils.data.DataLoader(val_dataset,   batch_size=batch_size, shuffle=False, drop_last=False, num_workers=0)\n",
    "    # ====== ラベル標準化（線形5次元のみ） ======\n",
    "    scaler = LabelScaler(use_log_linear=use_log_linear).fit(labels[np.array(train_indices)])\n",
    "    label_mu5_t, label_sigma5_t = scaler.to_torch(device)\n",
    "    # ====== モデル・損失・最適化・スケジューラ・EMA ======\n",
    "    model = PipeDimensionRegressor(out_dim=9, dropout_p=0.5, use_feature_stn=True).to(device)\n",
    "    criterion_linear = nn.SmoothL1Loss(beta=0.5)\n",
    "    criterion_angle  = nn.MSELoss()\n",
    "    optimizer = optim.AdamW(model.parameters(), lr=max_lr, weight_decay=weight_decay)\n",
    "    steps_per_epoch = max(1, len(train_loader))\n",
    "    scheduler = optim.lr_scheduler.OneCycleLR(\n",
    "        optimizer, max_lr=max_lr, epochs=num_epochs, steps_per_epoch=steps_per_epoch,\n",
    "        pct_start=0.1, anneal_strategy='cos', div_factor=10.0, final_div_factor=10.0\n",
    "    )\n",
    "    ema = ModelEMA(model, decay=0.999)\n",
    "    scaler_amp = torch.cuda.amp.GradScaler(enabled=(device.type == 'cuda'))\n",
    "    early_stopping = EarlyStopping(patience=patience_es, min_delta=0.0)\n",
    "    # ====== ログ ======\n",
    "    train_losses, val_losses = [], []\n",
    "    best_model_path = os.path.join(results_dir, \"best_model.pt\")\n",
    "    last_ckpt_path  = os.path.join(results_dir, \"last_checkpoint.pth\")\n",
    "    # ====== 学習ループ ======\n",
    "    for epoch in range(1, num_epochs + 1):\n",
    "        train_loss = train_one_epoch(\n",
    "            model, optimizer, train_loader, device,\n",
    "            label_mu5=label_mu5_t, label_sigma5=label_sigma5_t,\n",
    "            criterion_linear=criterion_linear, criterion_angle=criterion_angle,\n",
    "            scaler=scaler_amp,\n",
    "            max_grad_norm=max_grad_norm, ft_reg_weight=ft_reg_weight, angle_weight=angle_weight,\n",
    "            ema_updater=ema,\n",
    "            use_log_linear=use_log_linear\n",
    "        )\n",
    "        val_loss = evaluate(\n",
    "            ema.ema, val_loader, device,\n",
    "            label_mu5=label_mu5_t, label_sigma5=label_sigma5_t,\n",
    "            criterion_linear=criterion_linear, criterion_angle=criterion_angle,\n",
    "            ft_reg_weight=ft_reg_weight, angle_weight=angle_weight,\n",
    "            use_log_linear=use_log_linear\n",
    "        )\n",
    "        train_losses.append(train_loss)\n",
    "        val_losses.append(val_loss)\n",
    "        scheduler.step()\n",
    "        stop = early_stopping.step(val_loss, ema.ema, epoch)  # EMA重みを記録\n",
    "        if epoch % 20 == 0 or epoch == 1:\n",
    "            current_lr = optimizer.param_groups[0][\"lr\"]\n",
    "            print(f\"Epoch {epoch:04d} | LR {current_lr:.2e} | Train {train_loss:.6f} | Val {val_loss:.6f} | Best {early_stopping.best:.6f}\")\n",
    "        if epoch % 100 == 0 or epoch == num_epochs:\n",
    "            torch.save({\n",
    "                \"epoch\": epoch,\n",
    "                \"model_state_dict\": ema.state_dict(),  # EMA重みで保存\n",
    "                \"optimizer_state_dict\": optimizer.state_dict(),\n",
    "                \"best_val_loss\": early_stopping.best,\n",
    "            }, last_ckpt_path)\n",
    "        if stop:\n",
    "            print(f\"Early stopping at epoch {epoch}. Best epoch: {early_stopping.best_epoch}, Best val: {early_stopping.best:.6f}\")\n",
    "            break\n",
    "    # ベスト状態へ復元・保存（EMAのベスト状態）\n",
    "    if early_stopping.best_state is not None:\n",
    "        ema.ema.load_state_dict(early_stopping.best_state)\n",
    "    torch.save(ema.ema.state_dict(), best_model_path)\n",
    "    print(f\"Saved best EMA model to {best_model_path}\")\n",
    "    # 履歴CSV\n",
    "    hist_csv = os.path.join(results_dir, \"metrics_history.csv\")\n",
    "    with open(hist_csv, \"w\", newline=\"\") as f:\n",
    "        writer = csv.writer(f)\n",
    "        writer.writerow([\"epoch\", \"train_loss\", \"val_loss\"])\n",
    "        for e, (tr, va) in enumerate(zip(train_losses, val_losses), start=1):\n",
    "            writer.writerow([e, tr, va])\n",
    "    print(f\"Saved loss history to {hist_csv}\")\n",
    "    # Val（EMA重み、TTAあり）で7次元に復元して評価・保存（ラジアン・度の両方）\n",
    "    evaluate_and_save_casewise(\n",
    "        ema.ema,\n",
    "        val_case_ids=val_indices,\n",
    "        labels=labels,\n",
    "        device=device,\n",
    "        root_dir=\".\",\n",
    "        num_points=num_points,\n",
    "        out_csv=os.path.join(results_dir, \"val_predictions.csv\"),\n",
    "        out_json=os.path.join(results_dir, \"val_metrics.json\"),\n",
    "        tta_times=8,\n",
    "        rotate_axis=None,      # Valでは回転なし\n",
    "        preprocess_cfg=preprocess_cfg,\n",
    "        save_degree_metrics=True\n",
    "    )\n",
    "    # 損失曲線\n",
    "    epochs = np.arange(1, len(train_losses) + 1)\n",
    "    plt.figure(figsize=(7,5))\n",
    "    plt.plot(epochs, train_losses, label=\"Train Loss\", color=\"tab:blue\")\n",
    "    plt.plot(epochs, val_losses,   label=\"Val Loss (EMA)\",   color=\"tab:orange\")\n",
    "    plt.xlabel(\"Epoch\")\n",
    "    plt.ylabel(\"Composite Loss (Std linear + angle MSE + STN reg)\")\n",
    "    plt.title(\"Training and Validation Loss (EMA)\")\n",
    "    plt.grid(True, alpha=0.3)\n",
    "    plt.legend()\n",
    "    plt.tight_layout()\n",
    "    plt.savefig(os.path.join(results_dir, \"loss_curve.png\"), dpi=150)\n",
    "    # plt.show()\n",
    "if __name__ == \"__main__\":\n",
    "    main()\n",
    "\"\"\"\n",
    "使い方\n",
    "- 同ディレクトリに params_all.txt と pipe_case_XX.xyz 群を配置してください。\n",
    "- params_all.txt の角度単位に合わせて、angle_unit_labels を 'rad' または 'deg' に設定してください。\n",
    "- 実行: python Train_pipe8-2.py\n",
    "- 出力（results8-2 ディレクトリ）\n",
    "  - best_model.pt: EMAベストモデル（state_dict）\n",
    "  - last_checkpoint.pth: 直近チェックポイント（EMA重み）\n",
    "  - metrics_history.csv: 各エポックの Train/Val loss\n",
    "  - val_predictions.csv: Val 各ケースの予測・真値・絶対誤差（角度は最小差）\n",
    "  - val_metrics.json: MAE/MSE/RMSE（ラジアン・度の両方の要約）\n",
    "  - loss_curve.png: 学習・検証損失曲線\n",
    "補足（検証の着眼点）\n",
    "- augment_rotate=False と angle_unit_labels の整合を最優先で確認してください。\n",
    "- まだValが不安定な場合は、以下の追加を段階的に試してください。\n",
    "  - weight_decay を 1e-3 に上げる、dropout_p を 0.5 に上げる、ft_reg_weight を 0.02 に上げる\n",
    "  - num_points を 8192 に増やす、batch_size を増やす\n",
    "  - canonical_align=True（角度定義が主軸基準の場合のみ）\n",
    "- 交差検証（K-Fold）でスプリットの偏りをならして、by_dim_deg/by_dim_rad と best_val_loss の傾向を確認してください。\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "600c3cc5-4104-49f5-bc98-39a3fa474e67",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "結論\n",
    "- ほぼできますが、提示の追記コードだけでは load_model_from_weights が未定義のため動きません。\n",
    "- 下記のように「9次元モデル重みを読み込み、推論時に7次元（[L1..R2, theta1, theta2]）を返すラッパ」を備えた load_model_from_weights を実装して一緒に追記すれば、散布図を作成できます。\n",
    "- 併せて RUN_SCATTER=1 でCLIを有効化する if ブロックも追記します。\n",
    "ポイント\n",
    "- 学習済み重みは本編コードの通り results8-2/best_model.pt（state_dictのみ）または results8-2/last_checkpoint.pth（辞書形式）に対応します。\n",
    "- 9次元出力モデル（[L1..R2, s1, c1, s2, c2]）を内部で呼び、sincosを正規化→atan2で角度2次元に復元して7次元を返すラッパ SevenOutWrapper を用意します。\n",
    "- 散布図は角度をdegに変換して描画します。params_all.txt の角度単位に合わせて --angle-unit を設定してください。\n",
    "追記コード（Train_pipe8-2.py の末尾に貼り付け）\n",
    "- 先にお示しの「散布図モジュール」の前に、互換ローダー load_model_from_weights と7次元ラッパを定義します。\n",
    "- その後、いただいた散布図モジュールを置き、最後に RUN_SCATTER=1 のときだけCLIが起動する if ブロックを有効化します。\n",
    "\"\"\"\n",
    "# ========= ここから下を Train_pipe8-2.py の末尾に追記 =========\n",
    "# 9→7次元互換ローダーと7次元ラッパ\n",
    "import os as _os\n",
    "import torch as _torch\n",
    "import torch.nn.functional as _F\n",
    "class _SevenOutWrapper(_torch.nn.Module):\n",
    "    \"\"\"\n",
    "    内部に PipeDimensionRegressor(out_dim=9) を持ち、\n",
    "    forwardで (B,7) = [L1,L2,L3,R1,R2,theta1,theta2] を返すラッパ\n",
    "    \"\"\"\n",
    "    def __init__(self, base9):\n",
    "        super().__init__()\n",
    "        self.base9 = base9\n",
    "    def forward(self, x):\n",
    "        out9, _, _ = self.base9(x)  # (B,9)\n",
    "        lin5 = out9[:, :5]          # (B,5)\n",
    "        sc   = out9[:, 5:9]         # (B,4) = [s1,c1,s2,c2]\n",
    "        v1 = _F.normalize(sc[:, 0:2], dim=1, eps=1e-6)\n",
    "        v2 = _F.normalize(sc[:, 2:4], dim=1, eps=1e-6)\n",
    "        s1, c1 = v1[:, 0], v1[:, 1]\n",
    "        s2, c2 = v2[:, 0], v2[:, 1]\n",
    "        th1 = _torch.atan2(s1, c1).unsqueeze(1)\n",
    "        th2 = _torch.atan2(s2, c2).unsqueeze(1)\n",
    "        y = _torch.cat([lin5, th1, th2], dim=1)  # (B,7)\n",
    "        return y\n",
    "def load_model_from_weights(weights_path, device=None, out_dim=7):\n",
    "    \"\"\"\n",
    "    weights_path: results8-2/best_model.pt（state_dict）または results8-2/last_checkpoint.pth（辞書）\n",
    "    out_dim=7 を指定すると、9次元モデルを内部でロードし、_SevenOutWrapper で7次元出力に変換して返す。\n",
    "    \"\"\"\n",
    "    device = device or _torch.device(\"cuda\" if _torch.cuda.is_available() else \"cpu\")\n",
    "    # 9次元ベースモデルを構築\n",
    "    base = PipeDimensionRegressor(out_dim=9, dropout_p=0.5, use_feature_stn=True).to(device)\n",
    "    # 重みロード\n",
    "    ckpt = _torch.load(weights_path, map_location=device)\n",
    "    if isinstance(ckpt, dict) and \"model_state_dict\" in ckpt:\n",
    "        state = ckpt[\"model_state_dict\"]\n",
    "    elif isinstance(ckpt, dict) and \"state_dict\" in ckpt:\n",
    "        state = ckpt[\"state_dict\"]\n",
    "    else:\n",
    "        # best_model.pt は state_dict そのものを保存している想定\n",
    "        state = ckpt\n",
    "    missing, unexpected = base.load_state_dict(state, strict=False)\n",
    "    if missing or unexpected:\n",
    "        print(f\"[Warn] load_state_dict strict=False | missing={len(missing)} unexpected={len(unexpected)}\")\n",
    "    base.eval()\n",
    "    if out_dim == 7:\n",
    "        model = _SevenOutWrapper(base).to(device)\n",
    "    else:\n",
    "        model = base\n",
    "    model.eval()\n",
    "    return model\n",
    "# ========= ここから：散布図ユーティリティ（ご提示案を反映） =========\n",
    "import re as _re\n",
    "import argparse as _argparse\n",
    "import numpy as _np\n",
    "import matplotlib.pyplot as _plt\n",
    "# すでにTrain_pipe8-2.py内に以下が定義済みである前提：\n",
    "# - load_params_all_to_vec7\n",
    "# - adjust_label_units\n",
    "# - preprocess_points_np\n",
    "# - farthest_point_sampling\n",
    "def _extract_case_id(path):\n",
    "    m = _re.search(r\"pipe_case_(\\d+)\\.xyz\", _os.path.basename(path))\n",
    "    if not m:\n",
    "        raise ValueError(f\"Failed to parse case_id from filename: {path}\")\n",
    "    return int(m.group(1))\n",
    "@_torch.no_grad()\n",
    "def _predict7_single_file(model, path, device, num_points=4096,\n",
    "                          preprocess_cfg=None, tta_times=8, rotate_axis=None):\n",
    "    \"\"\"\n",
    "    単一ファイルに対して、前処理＋TTAを行い (7,) の予測を返す。\n",
    "    load_model_from_weights を使っておけば model(x) は常に (B,7) なので安全。\n",
    "    \"\"\"\n",
    "    preprocess_cfg = preprocess_cfg or {}\n",
    "    try:\n",
    "        pts = _np.loadtxt(path, skiprows=1)\n",
    "    except Exception:\n",
    "        pts = _np.loadtxt(path)\n",
    "    if pts.ndim != 2 or pts.shape[1] != 3:\n",
    "        raise ValueError(f\"Invalid point file shape: {path}, got {pts.shape}\")\n",
    "    pts = pts.astype(_np.float32)\n",
    "    preds = []\n",
    "    for _ in range(tta_times):\n",
    "        pts_pp = preprocess_points_np(\n",
    "            pts,\n",
    "            coord_scale=float(preprocess_cfg.get(\"coord_scale\", 1.0)),\n",
    "            dedup_round_decimals=int(preprocess_cfg.get(\"dedup_round_decimals\", 6)),\n",
    "            use_sor=bool(preprocess_cfg.get(\"use_sor\", True)),\n",
    "            sor_k=int(preprocess_cfg.get(\"sor_k\", 16)),\n",
    "            sor_std_ratio=float(preprocess_cfg.get(\"sor_std_ratio\", 2.0)),\n",
    "            canonical_align=bool(preprocess_cfg.get(\"canonical_align\", False)),\n",
    "            normalize_method=str(preprocess_cfg.get(\"normalize_method\", \"unit_sphere\")).lower()\n",
    "        )\n",
    "        pts_s = farthest_point_sampling(pts_pp, num_points)\n",
    "        if rotate_axis == 'z':\n",
    "            theta = _np.random.uniform(0.0, 2.0*_np.pi)\n",
    "            c, s = _np.cos(theta), _np.sin(theta)\n",
    "            R = _np.array([[c, -s, 0.0],[s, c, 0.0],[0.0,0.0,1.0]], dtype=_np.float32)\n",
    "            pts_s = pts_s @ R.T\n",
    "        x = _torch.from_numpy(pts_s.astype(_np.float32)).transpose(0,1).unsqueeze(0).to(device)  # (1,3,N)\n",
    "        pred7 = model(x)  # 常に (B,7)\n",
    "        pred7 = pred7.squeeze(0).cpu().numpy().reshape(-1).astype(_np.float32)\n",
    "        preds.append(pred7)\n",
    "    return _np.mean(_np.stack(preds, axis=0), axis=0)  # (7,)\n",
    "def scatter_plot(files, weights_path=\"results8-2/best_model.pt\",\n",
    "                 angle_unit_labels='rad', label_length_scale=1.0,\n",
    "                 preprocess_cfg=None, num_points=4096, tta_times=8,\n",
    "                 rotate_axis=None, out_path=\"results8-2/scatter_pred_vs_true.png\"):\n",
    "    \"\"\"\n",
    "    指定ファイル群について、真値（x） vs 予測（y）の散布図を作成・保存。\n",
    "    - files: [\"pipe_case_07.xyz\", ...]\n",
    "    - weights_path: 学習済み重み（results8-2/best_model.pt など）\n",
    "    - angle_unit_labels: 'rad' or 'deg'（params_all.txtの角度単位）\n",
    "    - label_length_scale: ラベル長さの単位スケール（例: mm->m は 0.001）\n",
    "    - preprocess_cfg: 学習時と同じ前処理設定（coord_scaleなど）\n",
    "    - rotate_axis: TTA用の回転軸（通常 None）\n",
    "    \"\"\"\n",
    "    preprocess_cfg = preprocess_cfg or {\n",
    "        \"coord_scale\": 1.0,\n",
    "        \"dedup_round_decimals\": 6,\n",
    "        \"use_sor\": True,\n",
    "        \"sor_k\": 16,\n",
    "        \"sor_std_ratio\": 2.0,\n",
    "        \"canonical_align\": False,\n",
    "        \"normalize_method\": \"unit_sphere\",\n",
    "    }\n",
    "    # ラベル読込＋単位変換\n",
    "    labels_raw, _ = load_params_all_to_vec7(\"params_all.txt\")\n",
    "    labels = adjust_label_units(labels_raw,\n",
    "                                angle_unit=angle_unit_labels,\n",
    "                                length_scale=label_length_scale,\n",
    "                                wrap_angles=True)\n",
    "    # 真値\n",
    "    sel_ids = [_extract_case_id(p) for p in files]\n",
    "    trues = _np.stack([labels[cid] for cid in sel_ids], axis=0)  # (B,7)\n",
    "    # モデル（7次元出力に統一）\n",
    "    device = _torch.device(\"cuda\" if _torch.cuda.is_available() else \"cpu\")\n",
    "    model = load_model_from_weights(weights_path, device=device, out_dim=7).to(device)\n",
    "    model.eval()\n",
    "    # 予測\n",
    "    preds = []\n",
    "    for p in files:\n",
    "        pred7 = _predict7_single_file(model, p, device=device, num_points=num_points,\n",
    "                                      preprocess_cfg=preprocess_cfg, tta_times=tta_times, rotate_axis=rotate_axis)\n",
    "        preds.append(pred7)\n",
    "    preds = _np.stack(preds, axis=0)  # (B,7)\n",
    "    # 散布図\n",
    "    keys = [\"L1\", \"L2\", \"L3\", \"R1\", \"R2\", \"theta1\", \"theta2\"]\n",
    "    B = preds.shape[0]\n",
    "    fig, axes = _plt.subplots(2, 4, figsize=(12, 6))\n",
    "    axes = axes.flatten()\n",
    "    for i, k in enumerate(keys):\n",
    "        ax = axes[i]\n",
    "        x = trues[:, i].copy()\n",
    "        y = preds[:, i].copy()\n",
    "        # 角度は度（deg）で描画\n",
    "        if i >= 5:\n",
    "            x = _np.degrees(x)\n",
    "            y = _np.degrees(y)\n",
    "            ax.set_xlabel(f\"True {k} (deg)\")\n",
    "            ax.set_ylabel(f\"Pred {k} (deg)\")\n",
    "        else:\n",
    "            ax.set_xlabel(f\"True {k}\")\n",
    "            ax.set_ylabel(f\"Pred {k}\")\n",
    "        ax.scatter(x, y, c=\"tab:blue\", s=50, alpha=0.85, edgecolors=\"white\", linewidths=0.6)\n",
    "        minv = min(_np.min(x), _np.min(y))\n",
    "        maxv = max(_np.max(x), _np.max(y))\n",
    "        ax.plot([minv, maxv], [minv, maxv], color=\"tab:gray\", linestyle=\"--\", linewidth=1.0)\n",
    "        if B >= 2:\n",
    "            r = _np.corrcoef(x, y)[0, 1]\n",
    "            ax.set_title(f\"{k} (r={r:.3f})\")\n",
    "        else:\n",
    "            ax.set_title(k)\n",
    "        ax.grid(True, alpha=0.3)\n",
    "    axes[-1].axis(\"off\")\n",
    "    _plt.tight_layout()\n",
    "    _os.makedirs(_os.path.dirname(out_path), exist_ok=True)\n",
    "    _plt.savefig(out_path, dpi=150)\n",
    "    _plt.show()\n",
    "    print(f\"Saved scatter figure to {out_path}\")\n",
    "def _scatter_cli():\n",
    "    \"\"\"\n",
    "    コマンドラインインターフェース。\n",
    "    既存の学習 main と干渉しないよう、環境変数 RUN_SCATTER=1 のときのみ起動します。\n",
    "    \"\"\"\n",
    "    parser = _argparse.ArgumentParser(description=\"True vs Pred scatter plot\")\n",
    "    parser.add_argument(\"--files\", nargs=\"+\", required=True,\n",
    "                        help=\"Target xyz files (e.g., pipe_case_07.xyz pipe_case_08.xyz ...)\")\n",
    "    parser.add_argument(\"--weights\", default=\"results8-2/best_model.pt\",\n",
    "                        help=\"Path to trained weights\")\n",
    "    parser.add_argument(\"--angle-unit\", choices=[\"rad\",\"deg\"], default=\"rad\",\n",
    "                        help=\"Angle unit in params_all.txt\")\n",
    "    parser.add_argument(\"--label-length-scale\", type=float, default=1.0,\n",
    "                        help=\"Length scale for labels (e.g., mm->m is 0.001)\")\n",
    "    parser.add_argument(\"--coord-scale\", type=float, default=1.0,\n",
    "                        help=\"Coordinate scale for point cloud (e.g., mm->m is 0.001)\")\n",
    "    parser.add_argument(\"--canonical-align\", action=\"store_true\",\n",
    "                        help=\"Enable PCA canonical alignment\")\n",
    "    parser.add_argument(\"--normalize-method\", choices=[\"unit_sphere\",\"zscore\",\"none\"], default=\"unit_sphere\",\n",
    "                        help=\"Coordinate normalization method\")\n",
    "    parser.add_argument(\"--use-sor\", action=\"store_true\",\n",
    "                        help=\"Enable SOR outlier removal (default: enabled)\")\n",
    "    parser.add_argument(\"--no-sor\", action=\"store_true\",\n",
    "                        help=\"Disable SOR outlier removal\")\n",
    "    parser.add_argument(\"--sor-k\", type=int, default=16, help=\"SOR k-neighbors\")\n",
    "    parser.add_argument(\"--sor-std-ratio\", type=float, default=2.0, help=\"SOR std ratio\")\n",
    "    parser.add_argument(\"--num-points\", type=int, default=4096, help=\"Subsample points per cloud\")\n",
    "    parser.add_argument(\"--tta-times\", type=int, default=8, help=\"TTA samples\")\n",
    "    parser.add_argument(\"--rotate-axis\", choices=[\"none\",\"z\"], default=\"none\", help=\"Optional TTA rotation axis\")\n",
    "    parser.add_argument(\"--out\", default=\"results8-2/scatter_pred_vs_true.png\", help=\"Output path for figure\")\n",
    "    args = parser.parse_args()\n",
    "    preprocess_cfg = {\n",
    "        \"coord_scale\": args.coord_scale,\n",
    "        \"dedup_round_decimals\": 6,\n",
    "        \"use_sor\": False if args.no_sor else True if args.use_sor else True,\n",
    "        \"sor_k\": args.sor_k,\n",
    "        \"sor_std_ratio\": args.sor_std_ratio,\n",
    "        \"canonical_align\": args.canonical_align,\n",
    "        \"normalize_method\": args.normalize_method,\n",
    "    }\n",
    "    rotate_axis = None if args.rotate_axis == \"none\" else args.rotate_axis\n",
    "    scatter_plot(files=args.files,\n",
    "                 weights_path=args.weights,\n",
    "                 angle_unit_labels=args.angle_unit,\n",
    "                 label_length_scale=args.label_length_scale,\n",
    "                 preprocess_cfg=preprocess_cfg,\n",
    "                 num_points=args.num_points,\n",
    "                 tta_times=args.tta_times,\n",
    "                 rotate_axis=rotate_axis,\n",
    "                 out_path=args.out)\n",
    "# 環境変数 RUN_SCATTER=1 のときだけ、CLIを起動\n",
    "if __name__ == \"__main__\" and _os.environ.get(\"RUN_SCATTER\") == \"1\":\n",
    "    _scatter_cli()\n",
    "# ========= 追記ここまで =========\n",
    "\"\"\"\n",
    "使い方\n",
    "- まずは学習を実行して best_model.pt を作ります。\n",
    "  - python Train_pipe8-2.py\n",
    "- 散布図のみ実行（学習は走らせず、RUN_SCATTER=1 を付与）\n",
    "  - Windows: set RUN_SCATTER=1 && python Train_pipe8-2.py --files pipe_case_07.xyz pipe_case_08.xyz pipe_case_09.xyz --weights results8-2/best_model.pt --angle-unit rad\n",
    "  - Linux/Mac: RUN_SCATTER=1 python Train_pipe8-2.py --files pipe_case_07.xyz pipe_case_08.xyz pipe_case_09.xyz --weights results8-2/best_model.pt --angle-unit rad\n",
    "- よく使うオプション\n",
    "  - params_all.txt の角度が度: --angle-unit deg\n",
    "  - ラベル・座標がmm: --label-length-scale 0.001 --coord-scale 0.001\n",
    "  - 学習時にPCAアラインメント使用: --canonical-align\n",
    "  - 正規化: --normalize-method unit_sphere\n",
    "  - 出力先変更: --out results8-2/my_scatter.png\n",
    "補足\n",
    "- 9→7変換は推論時にのみ行います（学習は従来通り9次元で実施）。これにより「not enough values to unpack」系の不整合を回避できます。\n",
    "- 前処理（coord_scale／SOR／アラインメント／正規化）は学習時と揃えてください。揃っていないと散布図の精度が悪化します。\n",
    "\"\"\"\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "f53b1ef1-a5a2-4d8c-b651-968cd01b6cd3",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "scatter_plot(\n",
    "    files=['pipe_case_01.xyz','pipe_case_02.xyz','pipe_case_03.xyz'],\n",
    "    weights_path='results8-2/best_model.pt',\n",
    "    angle_unit_labels='rad',\n",
    "    label_length_scale=1.0,\n",
    "    preprocess_cfg={'coord_scale':1.0,'dedup_round_decimals':6,'use_sor':True,'sor_k':16,'sor_std_ratio':2.0,'canonical_align':False,'normalize_method':'unit_sphere'},\n",
    "    num_points=4096, tta_times=8, rotate_axis=None,\n",
    "    out_path='results8-2/scatter_train_pred_vs_true.png'\n",
    ")"
   ]
  }
 ],
 "metadata": {
  "application/vnd.databricks.v1+notebook": {
   "computePreferences": null,
   "dashboards": [],
   "environmentMetadata": {
    "base_environment": "",
    "environment_version": "3"
   },
   "inputWidgetPreferences": null,
   "language": "python",
   "notebookMetadata": {
    "pythonIndentUnit": 4
   },
   "notebookName": "Train_2CrvPipe.py",
   "widgets": {}
  },
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
